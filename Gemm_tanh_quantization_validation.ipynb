{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "72835db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic\n",
    "from torch import nn\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import math\n",
    "import genesys_quantized_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bbe4753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Do your print / debug stuff here\n",
    "        print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "284aa392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From FBGEMM\n",
    "# https://github.com/pytorch/FBGEMM/blob/8f1b8777745d412c10d254284a72d76357ac287a/include/fbgemm/QuantUtils.h#L45\n",
    "def clamp(src, precision=8, signed=False):\n",
    "    min_num = -(1 << (precision - 1)) if signed else 0\n",
    "    max_num = ((1 << (precision - 1)) - 1) if signed else (1 << precision) - 1;\n",
    "    \n",
    "    return np.minimum(np.maximum(src, min_num), max_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3445517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From FBGEMM\n",
    "# https://github.com/pytorch/FBGEMM/blob/8f1b8777745d412c10d254284a72d76357ac287a/include/fbgemm/QuantUtils.h#L62\n",
    "def quantize(src, zero_point, scale, precision=8, signed=False):\n",
    "    inv_scale = 1.0/scale\n",
    "    transformed_val = src * inv_scale;\n",
    "    transformed_val = zero_point + np.round(transformed_val)\n",
    "    result = clamp(transformed_val, precision, signed)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "10fd19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From FBGEMM\n",
    "# https://github.com/pytorch/FBGEMM/blob/8f1b8777745d412c10d254284a72d76357ac287a/include/fbgemm/QuantUtils.h#L147\n",
    "def dequantize(src, zero_point, scale):\n",
    "    result = scale * (src - zero_point)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c20913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origial Model:\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(32,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37bcd9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Quantize(scale=tensor([0.0039]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (1): QuantizedLinear(in_features=32, out_features=1, scale=0.0029710179660469294, zero_point=255, qscheme=torch.per_tensor_affine)\n",
       "  (2): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.quantization.observer import MovingAverageMinMaxObserver\n",
    "\n",
    "# Pytorch Quantization\n",
    "model.eval()\n",
    "m_pytorch = copy.deepcopy(model)\n",
    "m_pytorch.eval()\n",
    "#backend = \"x86\"\n",
    "\n",
    "\"\"\"Insert stubs\"\"\"\n",
    "m_pytorch_q = nn.Sequential(torch.quantization.QuantStub(), \n",
    "                  *m_pytorch, \n",
    "                  torch.quantization.DeQuantStub())\n",
    "\n",
    "\"\"\"Prepare\"\"\"\n",
    "# qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "# print(qconfig)\n",
    "\n",
    "# qconfig = torch.quantization.QConfig(\n",
    "#   activation=MovingAverageMinMaxObserver.with_args(qscheme=torch.per_tensor_symmetric, dtype=torch.quint8),\n",
    "#   weight=MovingAverageMinMaxObserver.with_args(qscheme=torch.per_tensor_symmetric, dtype=torch.qint8)\n",
    "# )\n",
    "\n",
    "qconfig = torch.quantization.QConfig(\n",
    "  activation=MovingAverageMinMaxObserver.with_args(qscheme=torch.per_tensor_affine, dtype=torch.quint8),\n",
    "  weight=MovingAverageMinMaxObserver.with_args(qscheme=torch.per_tensor_affine, dtype=torch.qint8)\n",
    ")\n",
    "\n",
    "m_pytorch_q.qconfig = qconfig\n",
    "#torch.per_tensor_symmetric\n",
    "torch.quantization.prepare(m_pytorch_q, inplace=True)\n",
    "\n",
    "with torch.inference_mode():\n",
    "  for _ in range(1000):\n",
    "    x = torch.rand(10, 32)\n",
    "    m_pytorch_q(x)\n",
    "    \n",
    "\"\"\"Convert\"\"\"\n",
    "torch.quantization.convert(m_pytorch_q, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d34c00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0714, -0.0013, -0.1288, -0.1416,  0.1288,  0.0089, -0.0115, -0.0026,\n",
      "          0.0319, -0.1377, -0.0319, -0.1046,  0.1492,  0.0306, -0.0880, -0.0969,\n",
      "          0.0000, -0.1301,  0.0013,  0.0727,  0.0421,  0.0000,  0.1416, -0.1250,\n",
      "         -0.0281, -0.1760,  0.1454, -0.1569, -0.1046,  0.0587, -0.1467,  0.0791]],\n",
      "       size=(1, 32), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0012753951596096158,\n",
      "       zero_point=10)\n",
      "[[ -46    9  -91 -101  111   17    1    8   35  -98  -15  -72  127   34\n",
      "   -59  -66   10  -92   11   67   43   10  121  -88  -12 -128  124 -113\n",
      "   -72   56 -105   72]]\n"
     ]
    }
   ],
   "source": [
    "print(m_pytorch_q[1].weight())\n",
    "weight_pytorch = torch.int_repr(m_pytorch_q[1].weight()).numpy()\n",
    "print(weight_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ecaf153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Scale:  0.0012753951596096158\n",
      "Weight Zero Ploint:  10\n",
      "Original weight:\n",
      "tensor([[-0.0719, -0.0007, -0.1291, -0.1409,  0.1288,  0.0089, -0.0111, -0.0029,\n",
      "          0.0321, -0.1373, -0.0313, -0.1050,  0.1499,  0.0312, -0.0876, -0.0971,\n",
      "         -0.0003, -0.1303,  0.0011,  0.0723,  0.0425,  0.0003,  0.1416, -0.1251,\n",
      "         -0.0284, -0.1754,  0.1456, -0.1567, -0.1041,  0.0588, -0.1464,  0.0795]])\n",
      "Quantzed weight:\n",
      "[[ -46.    9.  -91. -101.  111.   17.    1.    8.   35.  -98.  -15.  -72.\n",
      "   127.   34.  -59.  -66.   10.  -92.   11.   67.   43.   10.  121.  -88.\n",
      "   -12. -128.  124. -113.  -72.   56. -105.   72.]]\n",
      "Dequantzed weight:\n",
      "[[-0.07142213 -0.0012754  -0.1288149  -0.14156887  0.1288149   0.00892777\n",
      "  -0.01147856 -0.00255079  0.03188488 -0.13774268 -0.03188488 -0.10458241\n",
      "   0.14922123  0.03060948 -0.08800226 -0.09693003  0.         -0.13009031\n",
      "   0.0012754   0.07269753  0.04208804  0.          0.14156887 -0.12498873\n",
      "  -0.02805869 -0.17600453  0.14539506 -0.1568736  -0.10458241  0.05866818\n",
      "  -0.14667045  0.0790745 ]]\n"
     ]
    }
   ],
   "source": [
    "# Custome Quantization\n",
    "weight = model[0].weight.detach()\n",
    "weight_scale = m_pytorch_q[1].weight().q_scale()\n",
    "weight_zero_point = m_pytorch_q[1].weight().q_zero_point()\n",
    "dequantize_weight_pytorch = dequantize(weight_pytorch, scale=weight_scale, zero_point=weight_zero_point)\n",
    "# print(\"Dequantzed pytorch weight:\")\n",
    "# print(dequantize_weight_pytorch)\n",
    "print(\"Weight Scale: \", weight_scale)\n",
    "print(\"Weight Zero Ploint: \", weight_zero_point)\n",
    "print(\"Original weight:\")\n",
    "print(weight)\n",
    "weight_q = quantize(src=weight, scale=weight_scale, zero_point=weight_zero_point, signed=True).numpy()\n",
    "print(\"Quantzed weight:\")\n",
    "print(weight_q)\n",
    "print(\"Dequantzed weight:\")\n",
    "weight_dequant = dequantize(weight_q, scale=weight_scale, zero_point=weight_zero_point)\n",
    "print(weight_dequant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47df84ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Quantization Sucess!\n"
     ]
    }
   ],
   "source": [
    "# Check Weight Quantization\n",
    "diff = weight_pytorch - weight_q\n",
    "print(diff)\n",
    "print(\"Quantization Sucess!\") if (abs(np.max(diff)) <= 1) else print(\"Quantization failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab90ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/google/gemmlowp/blob/master/doc/quantization_example.cc#L210\n",
    "def quantizeMultiplierSmallerThanOne(real_multiplier):\n",
    "    right_shift = 0\n",
    "    while real_multiplier < 0.5:\n",
    "        real_multiplier *= 2.0\n",
    "        right_shift+=1\n",
    "        \n",
    "    quantized_multiplier = np.round(real_multiplier * (1 << 31))\n",
    "    quantized_multiplier = np.int64(quantized_multiplier)\n",
    "    assert quantized_multiplier <= (1<<31)\n",
    "    quantized_multiplier = np.int32(quantized_multiplier)\n",
    "    \n",
    "    if quantized_multiplier == (1 << 31):\n",
    "        quantized_multiplier /= 2\n",
    "        right_shift-=1\n",
    "    \n",
    "    assert right_shift >= 0\n",
    "    assert quantized_multiplier <= np.iinfo(quantized_multiplier.dtype).max\n",
    "    return quantized_multiplier, right_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0b25e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Test\n",
    "input_float = torch.rand(1,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2e81901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Scale:  0.003910441\n",
      "Input Zero Point:  0\n",
      "Input Float:\n",
      "tensor([[0.4966, 0.6472, 0.6732, 0.6343, 0.6171, 0.3961, 0.9619, 0.0564, 0.7982,\n",
      "         0.0521, 0.4522, 0.6521, 0.6271, 0.1004, 0.0307, 0.6566, 0.2897, 0.7411,\n",
      "         0.3003, 0.3013, 0.9382, 0.9840, 0.8076, 0.0075, 0.3455, 0.1321, 0.9654,\n",
      "         0.8554, 0.3008, 0.2634, 0.9033, 0.2811]])\n",
      "Input Quant:\n",
      "[[127. 166. 172. 162. 158. 101. 246.  14. 204.  13. 116. 167. 160.  26.\n",
      "    8. 168.  74. 190.  77.  77. 240. 252. 207.   2.  88.  34. 247. 219.\n",
      "   77.  67. 231.  72.]]\n",
      "Input Dequant:\n",
      "[[0.496626   0.6491332  0.67259586 0.63349146 0.61784965 0.39495453\n",
      "  0.9619685  0.05474617 0.79772997 0.05083573 0.45361114 0.6530436\n",
      "  0.62567055 0.10167146 0.03128353 0.65695405 0.28937262 0.74298376\n",
      "  0.30110395 0.30110395 0.9385058  0.98543113 0.8094613  0.00782088\n",
      "  0.3441188  0.13295498 0.9658789  0.85638654 0.30110395 0.26199955\n",
      "  0.90331185 0.28155175]]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# Custome output\n",
    "#######################\n",
    "# 1. Quantize Input:\n",
    "input_scale = m_pytorch_q[0].scale.numpy()[0]\n",
    "input_zero_point = m_pytorch_q[0].zero_point.numpy()[0]\n",
    "input_q = quantize(src=input_float, scale=input_scale, zero_point=input_zero_point, signed=False).numpy()\n",
    "print(\"Input Scale: \", input_scale)\n",
    "print(\"Input Zero Point: \", input_zero_point)\n",
    "print(\"Input Float:\")\n",
    "print(input_float)\n",
    "print(\"Input Quant:\")\n",
    "print(input_q)\n",
    "print(\"Input Dequant:\")\n",
    "input_dequant = dequantize(src=input_q, scale=input_scale, zero_point=input_zero_point)\n",
    "print(input_dequant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74fcbc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Full Precision Output:  [[-0.36835927]]\n",
      "Reference Quantization Output:  tensor([[-0.3714]])\n",
      "Output Scale:  0.0029710179660469294\n",
      "Output Zero Point:  255\n",
      "Quantized Float Output Output:  [[131.]]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# pytorch output\n",
    "output_qt = model(input_float).detach().numpy()\n",
    "output_q_pt = m_pytorch_q(input_float)\n",
    "print(\"Reference Full Precision Output: \", output_qt)\n",
    "print(\"Reference Quantization Output: \", output_q_pt)\n",
    "#######################\n",
    "\n",
    "output_scale = m_pytorch_q[1].scale\n",
    "output_zero_point = m_pytorch_q[1].zero_point\n",
    "print(\"Output Scale: \", output_scale)\n",
    "print(\"Output Zero Point: \", output_zero_point)\n",
    "output_quantized = quantize(src=output_qt, scale=output_scale, zero_point=output_zero_point, signed=False)\n",
    "print(\"Quantized Float Output Output: \", output_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7a71c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Gemm:\n",
      "tensor([[-0.2468]])\n",
      "Original Gemm Quantized:\n",
      "tensor([[172.]], dtype=torch.float64)\n",
      "-0.24954741794820284\n",
      "-0.24954741794820284\n",
      "[[-0.24954742]]\n"
     ]
    }
   ],
   "source": [
    "# 2. Gemm\n",
    "print(\"Original Gemm:\")\n",
    "gemm = input_float @ weight.reshape(32,1)\n",
    "print(gemm)                                \n",
    "gemm_quantized = quantize(src=gemm, scale=output_scale, zero_point=output_zero_point, precision=32, signed=False)\n",
    "print(\"Original Gemm Quantized:\")\n",
    "print(gemm_quantized)\n",
    "# Gemm original equation 1:\n",
    "q1=input_q[0]-input_zero_point\n",
    "q2=weight_q[0]-weight_zero_point\n",
    "S=input_scale * weight_scale\n",
    "print(q1 @ q2.transpose() * S)\n",
    "\n",
    "# Gemm original equation 2:\n",
    "sum_gemm = 0 \n",
    "for idx, x in enumerate(input_q[0]):\n",
    "    sum_gemm += (x-input_zero_point) * (weight_q[0][idx]-weight_zero_point)\n",
    "print(sum_gemm*S)\n",
    "\n",
    "# Gemm dequant\n",
    "gemm_dequant = input_dequant @ weight_dequant.transpose()\n",
    "print(gemm_dequant)\n",
    "\n",
    "# Integer Gemm \n",
    "gemm_out_q = (input_q @ weight_q.transpose()).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "14a8504d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "act_times_w_scale = input_scale * weight_scale\n",
    "M = (act_times_w_scale / output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "57c60010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias float: \n",
      "tensor([-0.1215])\n",
      "Bias quantized: \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "# # 3. Quantize Bias\n",
    "bias_float = model[0].bias.detach()\n",
    "bias_q = quantize(src=bias_float, scale=act_times_w_scale, zero_point=0, precision = 32, signed=False).numpy()\n",
    "print(\"Bias float: \")\n",
    "print(bias_float)\n",
    "print(\"Bias quantized: \")\n",
    "print(bias_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9fea72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fxpmath import Fxp\n",
    "\n",
    "# 3. M decompose\n",
    "_, right_shift = quantizeMultiplierSmallerThanOne(M)\n",
    "M0 = M*(2**right_shift)\n",
    "M0_fxp = Fxp(M0, signed=True, n_word=64, n_frac=32)\n",
    "\n",
    "N = input_float.shape[1]\n",
    "#print(N)\n",
    "NZ1Z2 = N * input_zero_point * weight_zero_point\n",
    "a1 = np.sum(input_q, axis=1)\n",
    "a2 = np.sum(weight_q, axis=1)\n",
    "Z1a2 = int(input_zero_point * a2[0])\n",
    "Z2a1 = int((weight_zero_point * a1)[0])\n",
    "# print(NZ1Z2)\n",
    "# print(Z1a2)\n",
    "# print(Z2a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8fe8cf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before  [[-8416]]\n",
      "After  [[-50036]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before \", gemm_out_q)\n",
    "for r in range(gemm_out_q.shape[0]):\n",
    "    gemm_out_q[r,:] = gemm_out_q[r,:] - Z1a2\n",
    "for c in range(gemm_out_q.shape[1]):\n",
    "    gemm_out_q[:,c] = gemm_out_q[:,c] - Z2a1\n",
    "print(\"After \", gemm_out_q)\n",
    "gemm_out_q = gemm_out_q + bias_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11ba7dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M0_fxp.info()\n",
    "# print(gemm_out_q*M0)\n",
    "# out_test = M0_fxp * integer\n",
    "# out_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c059227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-83.99391077]]\n",
      "Quantized Gemm\n",
      "[[-0.24956551]]\n",
      "Final Output:\n",
      "[[-0.24956551]]\n"
     ]
    }
   ],
   "source": [
    "# M0 here\n",
    "fxp_part = (M0_fxp*(NZ1Z2+gemm_out_q)) >> right_shift\n",
    "gemm_final_out_q = output_zero_point + fxp_part.astype(np.int32)\n",
    "print(fxp_part)\n",
    "\n",
    "#gemm_final_out_q = gemm_final_out_q + bias_q.numpy()\n",
    "\n",
    "gemm_final_out = dequantize(src=gemm_final_out_q, scale=output_scale, zero_point=output_zero_point)\n",
    "print(\"Quantized Gemm\")\n",
    "print(gemm_final_out)\n",
    "\n",
    "# # Bias Add:\n",
    "output_q = gemm_final_out_q\n",
    "\n",
    "# # Final Dequantize\n",
    "output = dequantize(src=output_q, scale=output_scale, zero_point=output_zero_point)\n",
    "print(\"Final Output:\")\n",
    "print(output)\n",
    "\n",
    "# Original Fucntion\n",
    "# q3 = np.round(gemm_out_q * M + output_zero_point + bias_q).numpy().astype(np.int32)\n",
    "# output_test = dequantize(src=q3, scale=output_scale, zero_point=output_zero_point)\n",
    "# print(q3)\n",
    "# print(output_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a9ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c9ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
