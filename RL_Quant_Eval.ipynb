{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0f1229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "#import gym_oscillator\n",
    "#import oscillator_cpp\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "652c7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"/Users/ShawnXu/research-local/RL_quantization_eval/rl-trained-agents/\"\n",
    "algo = \"PPO\"\n",
    "env_id = \"MountainCarContinuous-v0_1\"\n",
    "file_name = \"MountainCarContinuous-v0.zip\"\n",
    "\n",
    "n_timesteps = 50000\n",
    "\n",
    "model_path = base_model_path+algo+\"/\"+env_id+\"/\"+file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99404df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ShawnXu/opt/anaconda3/envs/stable-baseline3/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object learning_rate. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: an integer is required (got type bytes)\n",
      "  warnings.warn(\n",
      "/Users/ShawnXu/opt/anaconda3/envs/stable-baseline3/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: an integer is required (got type bytes)\n",
      "  warnings.warn(\n",
      "/Users/ShawnXu/opt/anaconda3/envs/stable-baseline3/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: an integer is required (got type bytes)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(model_path)\n",
    "model_q = PPO.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d0139ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(\"MountainCarContinuous-v0\", n_envs=1)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "285ef8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = model_q.policy.mlp_extractor.policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64251a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ShawnXu/opt/anaconda3/envs/stable-baseline3/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantStub(\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (1): Linear(\n",
       "    in_features=2, out_features=64, bias=True\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (2): Tanh()\n",
       "  (3): Linear(\n",
       "    in_features=64, out_features=64, bias=True\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (4): Tanh()\n",
       "  (5): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.quantization import quantize_dynamic\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "policy_net.eval()\n",
    "\n",
    "# Dynamic Quantization\n",
    "# model_quantized = quantize_dynamic(\n",
    "#     model=policy_net, qconfig_spec={nn.Linear}, dtype=torch.qint8, inplace=False\n",
    "# )\n",
    "\n",
    "# Static Quantization\n",
    "policy_net = nn.Sequential(torch.quantization.QuantStub(), \n",
    "                  *policy_net, \n",
    "                  torch.quantization.DeQuantStub())\n",
    "\n",
    "\"\"\"Prepare\"\"\"\n",
    "backend = \"x86\"\n",
    "policy_net.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.quantization.prepare(policy_net, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb3c673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Calibration:  Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): Linear(\n",
      "    in_features=2, out_features=64, bias=True\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (2): Tanh()\n",
      "  (3): Linear(\n",
      "    in_features=64, out_features=64, bias=True\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (4): Tanh()\n",
      "  (5): DeQuantStub()\n",
      ")\n",
      "After Calibration:  Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=-0.8889830112457275, max_val=2.3281033039093018)\n",
      "  )\n",
      "  (1): Linear(\n",
      "    in_features=2, out_features=64, bias=True\n",
      "    (activation_post_process): HistogramObserver(min_val=-1.032845139503479, max_val=7.005014896392822)\n",
      "  )\n",
      "  (2): Tanh()\n",
      "  (3): Linear(\n",
      "    in_features=64, out_features=64, bias=True\n",
      "    (activation_post_process): HistogramObserver(min_val=-0.9405579566955566, max_val=6.739593029022217)\n",
      "  )\n",
      "  (4): Tanh()\n",
      "  (5): DeQuantStub()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Quantize(scale=tensor([0.0072]), zero_point=tensor([124]), dtype=torch.quint8)\n",
       "  (1): QuantizedLinear(in_features=2, out_features=64, scale=0.01523539423942566, zero_point=67, qscheme=torch.per_channel_affine)\n",
       "  (2): Tanh()\n",
       "  (3): QuantizedLinear(in_features=64, out_features=64, scale=0.012549459002912045, zero_point=75, qscheme=torch.per_channel_affine)\n",
       "  (4): Tanh()\n",
       "  (5): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_q.policy.mlp_extractor.policy_net = policy_net\n",
    "print(\"Before Calibration: \", policy_net)\n",
    "\n",
    "# Calibration with enviorment \n",
    "with torch.inference_mode():\n",
    "    for _ in range(n_timesteps):\n",
    "        action, _ = model_q.predict(obs)\n",
    "        obs, reward, done, infos = env.step(action)\n",
    "print(\"After Calibration: \", policy_net)\n",
    "\n",
    "torch.quantization.convert(policy_net, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2da393bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Int represenation of weight\n",
    "#print(torch.int_repr(model_quantized[0].weight()))\n",
    "\n",
    "# qint8 representation of weight\n",
    "#print(model_quantized[0].weight())\n",
    "\n",
    "# Assigne quantized network back to model\n",
    "model_q.policy.mlp_extractor.policy_net = policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4f4a72f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for _ in range(n_timesteps):\n",
    "#     action, _states = model.predict(obs)\n",
    "#     obs, rewards, dones, info = env.step(action)\n",
    "#     print(\"Obs: \", obs)\n",
    "#     print(\"Action: \", action)\n",
    "#     print(\"Reward: \", rewards)\n",
    "    \n",
    "#     action_q, _states_q = model_q.predict(obs)\n",
    "#     obs_q, rewards_q, dones_q, info_q = env.step(action_q)\n",
    "#     print(\"Obs_q: \", obs_q)\n",
    "#     print(\"Action_q: \", action_q)\n",
    "#     print(\"Reward_q: \", rewards_q)\n",
    "    \n",
    "#     print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9816d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_scipy(p, q):\n",
    "    p = np.asarray(p, dtype=np.float32)\n",
    "    q = np.asarray(q, dtype=np.float32)\n",
    "    p = p.flatten()\n",
    "    q = q.flatten()\n",
    "    p[p==0] = np.finfo(float).eps\n",
    "    q[q==0] = np.finfo(float).eps\n",
    "    if(len(p)>1):\n",
    "        pg = ss.gaussian_kde(p)\n",
    "        qg = ss.gaussian_kde(q)\n",
    "        kl = ss.entropy(pg(p),qg(q))\n",
    "        print(\"p,q\",ss.entropy(pg(p),qg(q)))\n",
    "        print(\"len of p\",len(p))\n",
    "        return kl\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4ec98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of network input:  [[-0.7115588   0.00665341]]\n",
      "Episode Reward: -24.05\n",
      "Episode Length 949\n",
      "p,q 0.0014349046626484048\n",
      "len of p 949\n",
      "Episode Reward: -28.23\n",
      "Episode Length 999\n",
      "p,q 0.0045029488865046385\n",
      "len of p 999\n",
      "Episode Reward: -24.49\n",
      "Episode Length 999\n",
      "p,q 0.027690900455145454\n",
      "len of p 999\n",
      "Episode Reward: -27.19\n",
      "Episode Length 999\n",
      "p,q 0.0006996311277053205\n",
      "len of p 999\n",
      "Episode Reward: -25.15\n",
      "Episode Length 999\n",
      "p,q 0.00164830262245456\n",
      "len of p 999\n",
      "Episode Reward: -27.36\n",
      "Episode Length 999\n",
      "p,q 0.0007363660432974928\n",
      "len of p 999\n",
      "Episode Reward: -25.10\n",
      "Episode Length 999\n",
      "p,q 0.00196165537118772\n",
      "len of p 999\n",
      "Episode Reward: -24.95\n",
      "Episode Length 999\n",
      "p,q 0.0027245523460435375\n",
      "len of p 999\n",
      "Episode Reward: -27.06\n",
      "Episode Length 999\n",
      "p,q 0.0008633360153543564\n",
      "len of p 999\n",
      "Episode Reward: -24.53\n",
      "Episode Length 999\n",
      "p,q 0.012980655537360112\n",
      "len of p 999\n",
      "Episode Reward: -24.77\n",
      "Episode Length 999\n",
      "p,q 0.003474617991094223\n",
      "len of p 999\n",
      "Episode Reward: -24.80\n",
      "Episode Length 999\n",
      "p,q 0.0036353687506190378\n",
      "len of p 999\n",
      "Episode Reward: -25.08\n",
      "Episode Length 999\n",
      "p,q 0.0019899573477276788\n",
      "len of p 999\n",
      "Episode Reward: -24.48\n",
      "Episode Length 999\n",
      "p,q 0.03489133918194609\n",
      "len of p 999\n",
      "Episode Reward: -26.37\n",
      "Episode Length 999\n",
      "p,q 0.0012302952622034427\n",
      "len of p 999\n",
      "Episode Reward: -26.89\n",
      "Episode Length 999\n",
      "p,q 0.0009329210485248655\n",
      "len of p 999\n",
      "Episode Reward: -26.39\n",
      "Episode Length 999\n",
      "p,q 0.0011942839859657782\n",
      "len of p 999\n",
      "Episode Reward: -24.78\n",
      "Episode Length 999\n",
      "p,q 0.003566217831013916\n",
      "len of p 999\n",
      "Episode Reward: -25.28\n",
      "Episode Length 999\n",
      "p,q 0.001302572051214051\n",
      "len of p 999\n",
      "Episode Reward: -24.59\n",
      "Episode Length 999\n",
      "p,q 0.008493429961300722\n",
      "len of p 999\n",
      "Episode Reward: -24.49\n",
      "Episode Length 999\n",
      "p,q 0.027736281258219822\n",
      "len of p 999\n",
      "Episode Reward: -26.02\n",
      "Episode Length 999\n",
      "p,q 0.0013037838655758213\n",
      "len of p 999\n",
      "Episode Reward: -27.50\n",
      "Episode Length 999\n",
      "p,q 0.0006976586478058178\n",
      "len of p 999\n",
      "Episode Reward: -25.59\n",
      "Episode Length 999\n",
      "p,q 0.0011674769662240146\n",
      "len of p 999\n",
      "Episode Reward: -24.55\n",
      "Episode Length 999\n",
      "p,q 0.009873329339831543\n",
      "len of p 999\n",
      "Episode Reward: -25.87\n",
      "Episode Length 999\n",
      "p,q 0.0011880815540813764\n",
      "len of p 999\n",
      "Episode Reward: -26.83\n",
      "Episode Length 999\n",
      "p,q 0.0009390816313538344\n",
      "len of p 999\n",
      "Episode Reward: -25.06\n",
      "Episode Length 999\n",
      "p,q 0.0021915370360254473\n",
      "len of p 999\n",
      "Episode Reward: -25.73\n",
      "Episode Length 999\n",
      "p,q 0.0012347581784240566\n",
      "len of p 999\n",
      "Episode Reward: -24.77\n",
      "Episode Length 999\n",
      "p,q 0.003223811730963055\n",
      "len of p 999\n",
      "Episode Reward: -26.10\n",
      "Episode Length 999\n",
      "p,q 0.0013138075856292423\n",
      "len of p 999\n",
      "Episode Reward: -27.63\n",
      "Episode Length 999\n",
      "p,q 0.0007728152320341782\n",
      "len of p 999\n",
      "Episode Reward: -25.56\n",
      "Episode Length 999\n",
      "p,q 0.0011639977967470476\n",
      "len of p 999\n",
      "Episode Reward: -24.60\n",
      "Episode Length 999\n",
      "p,q 0.007608296352346899\n",
      "len of p 999\n",
      "Episode Reward: -24.73\n",
      "Episode Length 999\n",
      "p,q 0.0024854399616550723\n",
      "len of p 999\n",
      "Episode Reward: -24.48\n",
      "Episode Length 999\n",
      "p,q 0.03377149898244899\n",
      "len of p 999\n",
      "Episode Reward: -28.19\n",
      "Episode Length 999\n",
      "p,q 0.00404021594186214\n",
      "len of p 999\n",
      "Episode Reward: -25.26\n",
      "Episode Length 999\n",
      "p,q 0.0013892727151768164\n",
      "len of p 999\n",
      "Episode Reward: -27.86\n",
      "Episode Length 999\n",
      "p,q 0.0014665737452684061\n",
      "len of p 999\n",
      "Episode Reward: -25.45\n",
      "Episode Length 999\n",
      "p,q 0.0014805713703287936\n",
      "len of p 999\n",
      "Episode Reward: -26.92\n",
      "Episode Length 999\n",
      "p,q 0.0008954794166282211\n",
      "len of p 999\n",
      "Episode Reward: -25.14\n",
      "Episode Length 999\n",
      "p,q 0.0016829322262628319\n",
      "len of p 999\n",
      "Episode Reward: -28.18\n",
      "Episode Length 999\n",
      "p,q 0.004006351201723615\n",
      "len of p 999\n",
      "Episode Reward: -25.50\n",
      "Episode Length 999\n",
      "p,q 0.0012904995728743747\n",
      "len of p 999\n",
      "Episode Reward: -24.60\n",
      "Episode Length 999\n",
      "p,q 0.007532116542114346\n",
      "len of p 999\n",
      "Episode Reward: -26.54\n",
      "Episode Length 999\n",
      "p,q 0.0010872773939514298\n",
      "len of p 999\n",
      "Episode Reward: -24.75\n",
      "Episode Length 999\n",
      "p,q 0.0028283204985326683\n",
      "len of p 999\n",
      "Episode Reward: -25.86\n",
      "Episode Length 999\n",
      "p,q 0.0012084879522590553\n",
      "len of p 999\n",
      "Episode Reward: -25.39\n",
      "Episode Length 999\n",
      "p,q 0.001554362069555031\n",
      "len of p 999\n",
      "Episode Reward: -25.77\n",
      "Episode Length 999\n",
      "p,q 0.0012574221127047906\n",
      "len of p 999\n",
      "Success rate: nan%\n",
      "Mean reward: -25.73\n",
      "KL-Lists: [0.0014349046626484048, 0.0045029488865046385, 0.027690900455145454, 0.0006996311277053205, 0.00164830262245456, 0.0007363660432974928, 0.00196165537118772, 0.0027245523460435375, 0.0008633360153543564, 0.012980655537360112, 0.003474617991094223, 0.0036353687506190378, 0.0019899573477276788, 0.03489133918194609, 0.0012302952622034427, 0.0009329210485248655, 0.0011942839859657782, 0.003566217831013916, 0.001302572051214051, 0.008493429961300722, 0.027736281258219822, 0.0013037838655758213, 0.0006976586478058178, 0.0011674769662240146, 0.009873329339831543, 0.0011880815540813764, 0.0009390816313538344, 0.0021915370360254473, 0.0012347581784240566, 0.003223811730963055, 0.0013138075856292423, 0.0007728152320341782, 0.0011639977967470476, 0.007608296352346899, 0.0024854399616550723, 0.03377149898244899, 0.00404021594186214, 0.0013892727151768164, 0.0014665737452684061, 0.0014805713703287936, 0.0008954794166282211, 0.0016829322262628319, 0.004006351201723615, 0.0012904995728743747, 0.007532116542114346, 0.0010872773939514298, 0.0028283204985326683, 0.0012084879522590553, 0.001554362069555031, 0.0012574221127047906]\n",
      "Mean KL-Divergence: 0.00489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ShawnXu/opt/anaconda3/envs/stable-baseline3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/ShawnXu/opt/anaconda3/envs/stable-baseline3/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "successes = []\n",
    "action_q_list = []\n",
    "action_list = []\n",
    "kl_list = []\n",
    "deterministic = True\n",
    "episode_reward = 0.0 \n",
    "episode_rewards = []\n",
    "ep_len = 0\n",
    "is_atari = False\n",
    "verbose = 1\n",
    "\n",
    "print(\"Shape of network input: \", obs)\n",
    "\n",
    "for _ in range(n_timesteps):\n",
    "    action, _ = model.predict(obs, deterministic=deterministic)\n",
    "    action_q, _ = model_q.predict(obs, deterministic=deterministic)\n",
    "    action_list.append(action.flatten().tolist())\n",
    "    action_q_list.append(action_q.flatten().tolist())\n",
    "\n",
    "    # Random Agent\n",
    "    # action = [env.action_space.sample()]\n",
    "    # Clip Action to avoid out of bound errors\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "\n",
    "    # Take an action\n",
    "    obs, reward, done, infos = env.step(action)\n",
    "\n",
    "    episode_reward += reward[0]\n",
    "    ep_len += 1\n",
    "\n",
    "    # For atari the return reward is not the atari score\n",
    "    # so we have to get it from the infos dict\n",
    "    if is_atari and infos is not None and verbose >= 1:\n",
    "        episode_infos = infos[0].get('episode')\n",
    "        if episode_infos is not None:\n",
    "            print(\"Atari Episode Score: {:.2f}\".format(episode_infos['r']))\n",
    "            print(\"Atari Episode Length\", episode_infos['l'])\n",
    "\n",
    "            # calculate KL-divergence\n",
    "            flat_action = [item for sublist in action_list for item in sublist]\n",
    "            flat_action_q = [item for sublist in action_q_list for item in sublist]\n",
    "            kl_list.append(kl_scipy(flat_action, flat_action_q))\n",
    "            plt.hist(flat_action, bins=20, label='action')\n",
    "            plt.hist(flat_action_q, bins=20, label='action_q')\n",
    "            plt.legend()\n",
    "            # save the figure\n",
    "            # append the env-name to the file-name\n",
    "            # appen algo name to the file-name\n",
    "            plt.savefig(os.path.join('./action_hist_' + env_id + '_' + algo + '.png'))\n",
    "            plt.close()\n",
    "            flat_action = []\n",
    "            flat_action_q = []\n",
    "            action_list = []\n",
    "            action_q_list = []\n",
    "\n",
    "    if done and not is_atari and verbose > 0:\n",
    "        # NOTE: for env using VecNormalize, the mean reward\n",
    "        # is a normalized reward when `--norm_reward` flag is passed\n",
    "        print(\"Episode Reward: {:.2f}\".format(episode_reward))\n",
    "        print(\"Episode Length\", ep_len)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "        ep_len = 0\n",
    "\n",
    "        # calculate KL-divergence\n",
    "        flat_action = [item for sublist in action_list for item in sublist]\n",
    "        flat_action_q = [item for sublist in action_q_list for item in sublist]\n",
    "        kl_list.append(kl_scipy(flat_action, flat_action_q))\n",
    "        plt.hist(flat_action, bins=20, label='action')\n",
    "        plt.hist(flat_action_q, bins=20, label='action_q')\n",
    "        plt.legend()\n",
    "        # save the figure\n",
    "        # append the env-name to the file-name\n",
    "        plt.savefig(os.path.join('action_hist_' + env_id + '_' + algo + '.png'))\n",
    "        plt.close()\n",
    "        flat_action = []\n",
    "        flat_action_q = []\n",
    "        action_list = []\n",
    "        action_q_list = []\n",
    "\n",
    "    # Reset also when the goal is achieved when using HER\n",
    "    if done or infos[0].get('is_success', False):\n",
    "        if algo == 'her' and verbose > 1:\n",
    "            print(\"Success?\", infos[0].get('is_success', False))\n",
    "        # Alternatively, you can add a check to wait for the end of the episode\n",
    "        # if done:\n",
    "        obs = env.reset()\n",
    "        if algo == 'her':\n",
    "            successes.append(infos[0].get('is_success', False))\n",
    "            episode_reward, ep_len = 0.0, 0\n",
    "\n",
    "print(\"Success rate: {:.2f}%\".format(100 * np.mean(successes)))\n",
    "print(\"Mean reward: {:.2f}\".format(np.mean(episode_rewards)))\n",
    "\n",
    "env.close()\n",
    "\n",
    "# calculate kl-divergence over action dist\n",
    "# get the mean of a list\n",
    "print(\"KL-Lists:\", kl_list)\n",
    "mean_kl = np.mean(kl_list)\n",
    "print(\"Mean KL-Divergence: {:.5f}\".format(mean_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9badbee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
